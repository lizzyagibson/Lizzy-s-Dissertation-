\chapter[Introduction]{Introduction: Complex mixtures, complex analyses, an emphasis on interpretable results}
\vspace{-3em}

\begin{center}
Elizabeth A. Gibson,\textsuperscript{1} 
Jeff Goldsmith,\textsuperscript{2} 
Marianthi-Anna Kioumourtzoglou\textsuperscript{1} \\

\textbf{Affiliations:} \\ 
1. Department of Environmental Health Sciences, Columbia University; \\ 
2. Department of Biostatistics, Columbia University. \\ 

\textbf{Published as:} \\
Gibson EA, Goldsmith J, Kioumourtzoglou M-A. Complex Mixtures, Complex Analyses: an Emphasis on Interpretable Results. \textit{Current Environmental Health Reports}. 2019 May 8:1-9.
\end{center}
\label{sec:intro}

\clearpage

%% main text
\section{Background}\label{sec:Intro}

We are exposed daily to numerous environmental pollutants. Only a small proportion of these has been assessed for toxicity, with most studies conducted in experimental settings and not necessarily involving humans \citep{grandjean06}. Furthermore, studies evaluating adverse health have traditionally conducted single-chemical analyses. This approach, however, does not represent reality; we are exposed to a mixture of chemicals at any given time, which can act synergistically or antagonistically. Furthermore, due to high correlations among many of these chemicals, we might detect associations between some of them and the outcome of interest due to their correlation with the actual ``bad actor(s),'' i.e. the actual toxic agent(s) in the mixture. Finally, testing a plethora of chemicals in single-pollutant models---i.e., multiple comparisons---dramatically increases the chances of spurious findings and, consequently, may increase disagreement across studies. For these reasons, the US Environmental Protection Agency, National Research Council (NRC), and National Institute of Environmental Health Sciences have all recognized the necessity to assess exposure to mixtures \citep{epaSP, nrc}.

Assessing exposures to mixtures, nonetheless, is especially challenging. First, the dimensionality of the data dramatically increases when one includes multiple chemicals in the statistical model. Many studies do not have the power to accommodate this need. Furthermore, high correlation among chemicals can lead to collinearity and subsequently inflated standard errors and unstable effect estimates. Two main issues stemming from current limitations in mixtures analyses have been identified: the need for (a) novel and robust statistical approaches to assess exposure to mixtures, and (b) appropriate use of available statistical methods in epidemiologic studies \citep{taylor16}.

Given the increasing need to incorporate complex high-dimensional data in environmental health studies, researchers have progressively turned towards machine learning methods. Adapting machine learning and data science methods can be especially advantageous, leading to more comprehensive studies of environmental exposure impacts on human health. Nonetheless, these methods were developed to serve a different purpose, mostly focusing on optimizing predictive accuracy, which is not necessarily well-aligned with Public and Environmental Health. Environmental health researchers, therefore, should be especially cautious when using such methods, and preferably should work with computer and data scientists, in collaboration with biostatisticians, to best adapt and extend machine learning methods for appropriate use in environmental health. 

The goal of this paper is not to give a comprehensive overview of all existing methods to analyze exposure to mixtures. Instead, we will discuss four types of scientific questions that are of interest in mixtures research. We will provide a conceptual description of analytic techniques appropriate to answer each question, along with examples in recent studies. No single method, to date, can adequately address all four types of mixtures-related scientific questions \citep{taylor16}. Although other reviews exist on mixtures methods \citep{hamra2018environmental, stafoggia2017statistical, huang2018cumulative, coker2018multi}, here we emphasize the need for methods that ensure robust results while focusing on interpretability and inference. Although the specific research question(s) might differ across studies, the two aims of mixtures analyses are universal: to (1) better understand biological pathways of pathogenesis, and (2) inform maximally efficient targeted interventions and policies to best protect the public and prevent disease. For both of these aims, it is of utmost importance to select {\textbf robust} methods that provide {\textbf interpretable}, and therefore actionable, results.

\section{Complex mixture methods}\label{sec:Methods}

Generally, exposure to a mixture indicates exposure to multiple ``stressors'' simultaneously, which can include both chemical and non-chemical (e.g., socioeconomic status, diet, etc.) components. The question becomes, how can we represent the complexity of reality in a statistical model? 

The selected method(s) should be based on the primary research question. If the interest lies in identifying exposure patterns or groups of people with similar exposure profiles, some dimensionality reduction is required \citep{jolliffe02, thompson04, paatero94}. To identify the toxic agent(s) in a mixture, variable selection approaches may be more appropriate \citep{tibshirani96, zou05}. If the aim is to evaluate synergistic or antagonistic effects, the main options are to hard-code interactions into the health model or take advantage of more flexible semi- or non-parametric models \citep{bobb2014bayesian,coull2015,bobb2018statistical}. Finally, to observe the effect of the overall mixture, one may create a weighted index of exposure or compute the full posterior distribution using Bayesian methods \citep{carrico15,bobb2014bayesian,coull2015}.

We present the four main research questions most relevant for mixtures analyses in Table~\ref{tab:qx}. In the next sections, we describe appropriate methods to address each of these questions and provide applied examples. Please note that many of the methods discussed may answer multiple questions and thus fall under multiple subsections. To avoid repetition, we present applications under the research question to which they contribute most uniquely. \\

\begingroup
\renewcommand{\arraystretch}{1.4}
\begin{table}[ht]
\begin{center}
\caption[The four main questions in mixtures analyses]{The four main possible questions in mixtures analyses.}
\label{tab:qx}
\begin{tabular}{c p{15cm}}
\hline
\hline
  1. & Are there specific patterns of exposure in the study population? \\
  \hline
  2. & Which are the toxic agents in the mixture? Or, what are the independent effects of each mixture member on the health outcome of interest? \\
  \hline
  3. & Are there synergistic effects or interactions among mixture members? \\
  \hline
  4. & What is the overall effect of the mixture on the outcome of interest? \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}
\endgroup
\vspace{-2em}

\subsection{Pattern or profile identification}\label{sec:Patterns}

Identification of exposure patterns in the population, e.g. due to common sources or behaviors, is highly desirable if the goal is to inform targeted interventions and regulations. Once common patterns are identified, they can be included as the exposures of interest in health models, resulting in subsequent identification of the most toxic sources/behaviors. Regulatory agencies, then, can act on certain sources, and interventions can be designed to target specific behaviors. Methods adopted from the pattern recognition field are powerful tools to help researchers identify these shared exposure patterns. 

Questions about pattern or profile identification usually involve \textit{unsupervised} techniques to describe the variability among correlated chemicals in fewer unobserved (i.e., latent) factors or to identify subgroups of individuals with similar exposure profiles (i.e., clusters). The solution of unsupervised approaches is obtained independently of any outcome(s) of interest. Both clustering and factor analysis involve dimensionality reduction of the original data. Clustering groups study analysis units (e.g. participants in a cohort study or days in a time-series), and factor analysis techniques group chemicals into factors using combinations of the mixture members within each factor, i.e. patterns. To be meaningful, the number of clusters or patterns should be substantially lower in dimension than the original data. 

Clustering partitions observations (e.g., study participants) into distinct homogeneous groups so that observations within groups are similar and observations across groups are different. Clustering is often used in exploratory analyses, although the identified clusters can later be included in a health model as indicators. Though clustering is not particularly useful in estimating main effects, this approach can be advantageous when assessing effect modification by high-dimensional modifiers \citep{mak15_clusters}. Although the results from clustering can be quite interpretable, there is no ``golden rule'' for choosing the number of clusters \citep{ISLR}, highlighting the importance of expert knowledge in interpretation.

It may be more appropriate in environmental mixtures analyses to identify exposure patterns as functions of all mixture members representing specific sources of exposure or common behaviors in the study population. Pattern identification requires expert knowledge to assign interpretable labels to the estimated patterns. Principal component analysis (PCA) is the most commonly used dimension reduction technique employed in environmental epidemiology \citep{jolliffe02, pang16, mak14_unc, robinson2018urban, manzano2015tnf}. PCA aims to explain as much of the total variance in the data as possible using a smaller number of variables (called components), which are linear combinations of the original variables. The researcher must then decide the appropriate number of components to include in further analyses based on predefined criteria, by e.g. having {\textit a priori} defined a desired amount of the total variance explained. Although PCA is still widely used, its limitations include an orthogonal solution (which might be contrary to reality if the exposure patterns to be identified are not independent), no guarantee of an interpretable solution, and reliance on the researcher to decide on the number of components to retain for subsequent analyses.

While more advanced methods of matrix factorization exist \citep{candes2011robust} including positive matrix factorization (PMF) and sparse non-negative matrix underapproximation (SNMU), the structure of the results appears largely similar. PMF and SNMU are similar to traditional factor analysis in that the number of mixture components is designated by the researcher \citep{gillis2010, gillis2013, paatero94}, but they both include constraints in the matrix factorization that enhance interpretability. First, the non-negativity constraint in both PMF and SNMU ensures that individual scores and variable loadings on factors are on the same range as the original variables \citep{lee2001, paatero94}, as all environmental data are positive (e.g. chemical concentrations). The factors and individual exposures can be easily described---factors by the relative proportions of variables, and individual exposures by the relative proportions of factors. Second, both PMF and SNMU, unlike PCA, provide non-orthogonal results which can more realistically describe human exposure \citep{paatero94, lee2001}. Finally, SNMU adds a sparsity constraint on the solution by including a penalty term forcing the lowest contributing variables in the factor loadings to zero, ignoring chemicals that do not add to the mixture \citep{gillis2013}.

Traor\'{e} et al. implemented SNMU to identify mixtures of 210 environmental contaminants, including pesticide residues, trace elements and minerals, in two cohorts of pregnant women in France \citep{traore2018}. The authors selected the optimal number of mixture components in terms of relevance and quality of interpretation, choosing eight \citep{traore2018}. They additionally applied hierarchical clustering to identify groups of women with similar co-exposure profiles \citep{traore2018}, clustering participants based on the patterns identified by the SNMU.

\subsection{Identification of toxic agents and independent effects}\label{sec:Ind}

When interested in the identification of specific toxic agents within a mixture and the characterization of their exposure-response curves, the method of choice should help us estimate the independent effects of each mixture member. Any analysis, therefore, should incorporate information on the outcome of interest (i.e., {\textit supervised} approaches).

Variable selection is one family of methods that may aid in identifying toxic agents by choosing a subset of relevant mixture members. The most traditional form is subset selection, including automated forward and backward selection and best subset selection \citep{ISLR}. While these are easy to implement, they can be unstable, as small changes in the data can greatly affect variable inclusion in the model, and the uncertainty in the variable selection portion is ignored \citep{shen2002adaptive, fan2001variable}, resulting in an increased type I error rate \citep{leamer1978specification, raftery1996approximate, draper1995assessment}.

To address flaws in subset selection, penalized regression techniques can be used; these out-perform ordinary least squares (OLS) in their predictive capacity. Notably, penalized regression methods perform better in highly correlated settings, finding a unique solution even when the number of chemicals is larger than the number of observations \citep{fan2006statistical}. These methods cannot, as no method can, determine causal agents in highly correlated mixtures, but they continue to predict well in these settings, where OLS would provide unstable effect estimates and inflated standard errors. 

By penalizing the magnitude of the coefficients, ``unimportant'' variables shrink toward zero, i.e., their estimated effects are restricted, allowing estimation of the coefficients that are more strongly associated with the outcome. This trades some bias in the estimated coefficients for lower variance and overall mean squared error (MSE) of the predicted outcome.

Multiple penalization forms exist. Ridge regression shrinks the sum of the squares of the coefficients, resulting in non-zero coefficients that are smaller than or equal to those that would have been obtained using OLS \citep{hoerl1970ridge}. Lasso (Least absolute shrinkage and selection operator) shrinks the sum of the absolute values of the coefficients, which pushes some coefficients to zero, yielding a sparse solution \citep{tibshirani96}. Elastic net includes both penalization terms \citep{zou05}.

The penalization term in each above-mentioned approach includes a tuning parameter ($\lambda$) between zero (making the model equivalent to OLS) and infinity (where all coefficients are shrunk towards zero) \citep{friedman2001elements, ISLR}. Usually, model fitting includes a training set and a validation set to choose $\lambda$, followed by a test set to estimate the true MSE of the model \citep{daume2012course}. In environmental epidemiology, a test set may not be necessary and is often unavailable, but some form of hold-out or cross-validation analysis to justify the choice of $\lambda$ is warranted.

Lasso is more commonly used than ridge regression recently because it produces a sparse solution. It has been shown to outperform other penalized methods when there is a small to moderate number of moderate-sized true effects, while ridge regression performs better when there is a large number of small true effects \citep{tibshirani96}. When mixture members are highly correlated, ridge and elastic net will push coefficients toward each other \citep{zou05, hoerl1970ridge}; lasso will keep one of the correlated variables in the model and push the others to zero \citep{tibshirani96}. If multiple toxic agents in correlated mixtures are hypothesized, elastic net may provide the best balance of sparsity and inclusion of correlated variables that best predict the health outcome.

The coefficients for the selected variables are not necessarily the same as those that would have been obtained from OLS including only that subset. It is even possible for corresponding coefficients in the two models to be in different directions \citep{tibshirani96}. A large drawback for use of these penalization methods in environmental health is the difficulty in obtaining valid inferences, as the coefficients are non-linear and non-differentiable \citep{tibshirani96}. To overcome this, many researchers have first fit a penalized regression (e.g. Lasso) and subsequently included the selected variables in an OLS model. This practice is not well justified for inference, as it underestimates standard errors by ignoring uncertainty in the variable selection step.

Nwanaji-Enwerem et al. used an adaptive lasso to select PM$_{2.5}$ constituents associated with DNA methylation age \citep{nwanaji2017associations}. This approach incorporates user-specified weights to penalize individual coefficients differently, so that constituents with larger effects are penalized less than those with smaller effects \citep{zou2006adaptive}. The mixture of interest in their analysis included five PM$_{2.5}$ constituents (organic and elemental carbon, sulfate, nitrate, and ammonium), that made up 89\% of the total PM$_{2.5}$ mass concentration. With covariates fixed in the model so that only constituents could be penalized, sulfate and ammonium remained in the model, positively predicting Horvath DNA methylation age \citep{nwanaji2017associations}.

\subsection{Interactions}\label{sec:Interact}

Identification of potentially synergistic effects among chemicals is essential if there is reason to believe that the combined health effect is greater (or less) than the sum of the independent effects. This is often hypothesized when studying chemicals that share stereo-chemical features or that target the same biological pathway. If regulatory action or interventions aim only to lower exposure to one chemical below a certain threshold, while this chemical works synergistically with another, then the necessary reduction will be underestimated among people exposed to both chemicals. Methods to assess interactions between chemicals can identify susceptible groups in those exposed to interacting chemicals simultaneously. Interactions can be hard-coded into models, including lasso and weighted quantile sum (WQS) regression (see Section~\ref{sec:Overall}). However, this practice requires {\textit a priori} deciding which interaction terms to include and can only accommodate a small number of all potential high-order and non-linear interactions. To address this limitation, semi- or non-parametric methods are preferred.

Non-parametric methods make no assumptions about the functional form of the association, instead using tuning parameters to estimate a curve as closely as possible to each point without overfitting \citep{ISLR}. Such approaches can more accurately fit nonlinear exposure-response relationships and allow for non-additive interactions among all mixture members without explicitly including them in the model. Semi-parametric methods combine the flexibility of non-parametric models with a parametric portion which is computationally easier to estimate \citep{friedman2001elements}, allowing for the adjustment of potential confounders. However, such approaches often require a larger sample size than is typically needed for a parametric approach, since they do not reduce the problem of estimating the functional form of the data to a few parameters \citep{ISLR}.

Bayesian kernel machine regression (BKMR) is a semi-parametric technique that models the exposure-response relationship as a non-parametric kernel function of the mixture members, adjusting for covariates parametrically \citep{bobb2014bayesian, coull2015,bobb2018statistical}. The Gaussian kernel is commonly used for flexibly capturing a wide range of underlying functional forms, including non-additive interactions, without specifying the shape of the individual exposure-response curves or the existence of interactions among mixture members \citep{bobb2014bayesian, liu2007semiparametric}. BKMR also assesses independent effects, allows for component-wise or hierarchical variable selection, and estimates the overall effect of a mixture \citep{bobb2014bayesian, coull2015, bobb2018statistical}, but we include it in this section due to its unique ability to detect nonlinear interactions.

Wasserman et al. used BKMR to estimate the joint effects of exposure to a mixture of five metals (arsenic, lead, manganese, cadmium, and selenium, measured cross-sectionally) and peri-natal arsenic on intellectual function in adolescents in Bangladesh \citep{wasserman2018cross}. While no interactions were observed, they found increased arsenic and cadmium were associated with decreased raw full scale IQ, as was the overall mixture exposure \citep{wasserman2018cross}.

Other methods to assess high-order and non-linear interactions include tree-based methods \citep{friedman2001elements}. Regression and classification decision trees yield highly interpretable results, but they tend to be unstable, i.e., small changes in the data can cause large changes in the estimated trees. More complex tree-based methods, such as random forests, are more robust to variation and have improved prediction, but they lose the interpretability of the single tree \citep{ISLR}. Several groups have begun to implement these methods in environmental mixtures \citep{stingone2017using, ouidir2017atmospheric, gass2014classification}.

\subsection{Overall mixture effect}\label{sec:Overall}

Characterizing the overall effect of combined chemical exposures is necessary to adequately define the total body burden of environmental mixtures. When exposure to individual compounds is below a set regulatory concentration or too low to show independent effects, an overall effect may still exist in combination with other exposures which target a common health endpoint. The NRC now recommends that risk assessment efforts account for cumulative risk associated with chemicals that affect the same health outcome \citep{national2009phthalates, huang2018cumulative}. If no interaction is present, i.e., effects are believed to be additive, a composite of chemicals or a weighted index allows for the estimation of the combined effects of individual compounds without reducing the unique exposures to a simple sum. 

Various methods exist to create a weighted score of exposure prior to the modeling step. Toxic equivalency factors (TEF), for example, are often used with dioxins and dioxin-like chemicals to weigh their toxicity in terms of the most toxic dioxin. Individual weights are determined by structural and binding similarities, ability to elicit a toxic response, persistence, and bio-magnification. A single number---a toxic equivalency (TEQ) score---is estimated as the sum of the products of each chemical's concentration and its individual TEF value, and can be used as a cumulative measure of exposure to these related chemicals \citep{van20062005, mitro2015cross}. Use of TEQ, however, is limited to chemicals whose main mechanism of action is shared with dioxin. Creating such indices, therefore, for other mixtures can be challenging, especially if such prior knowledge is not available.

When less is known \textit{a priori} about the individual toxicity of the mixture members, WQS regression creates an empirically weighted index which can be more widely implemented for any mixture. The estimated coefficient of this index is interpreted as the mixture effect \citep{carrico15}. As the name implies, WQS categorizes the continuous exposures into quantiles to reduce the impact of outliers and ensure that all exposure variables are on the same scale \citep{carrico15, gennings2013cohort}. but this also reduces the amount of information in the data. WQS is analogous to the variable selection methods discussed in Section~\ref{sec:Ind}, with each variable's penalization determined by its respective weight. WQS then assigns a single coefficient to the weighted index---the sum of the concentration quantiles of each member multiplied by its weight. The weights identify toxic agents and ``zero out'' chemicals with negligible associations \citep{carrico15, christensen2013multiple}. If the index coefficient is statistically significant, important components of the index (i.e., toxic agents) can be identified as those with the highest weights \citep{carrico15} The weights provide information on the relative importance of individual mixture members but no corresponding effect estimates. 

White et al. used WQS to estimate the overall effect of a mixture of ten metals (antimony, arsenic, cadmium, chromium, cobalt, lead, manganese, mercury, nickel, selenium) on breast cancer risk \citep{white2018metallic}. The WQS index was positively associated with postmenopausal breast cancer but not with overall or ER+ breast cancer. Cadmium, lead, and mercury had the largest weights in the postmenopausal breast cancer index \citep{white2018metallic}.

\section{Bayesian methods}\label{sec:Bayes}
Even though challenges still remain, recent advances in computational performance and scalability have opened the door to Bayesian methods in environmental epidemiology \citep{hoffman2013stochastic}. Bayesian methods explicitly use probability to quantify uncertainty in inference, i.e., there is (in principle) no impediment to fitting models with many parameters, correlated exposure variables, or complicated exposure-response specifications \citep{bda3}, and these methods may be used to answer multiple mixtures questions in the same analysis. Given the flexibility of Bayesian methods, they are a promising direction for new development.

Bayesian methods estimate the full posterior distribution of the unobserved quantities \citep{bda3}, meaning that all Bayesian models can estimate an overall effect. Additionally, inclusion of prior information---a hallmark of Bayesian data analysis---becomes a powerful tool in environmental mixture methods. Prior knowledge of effect estimates (magnitude or direction taken from expert knowledge or previous research) or chemical groupings (by exposure source, biological pathway, or shared toxicity) can be explicitly incorporated in the model.

BKMR, for example, assesses potentially non-linear independent effects and the overall effect in addition to interactions among mixture members. It also allows for hierarchical grouping of mixture members \citep{bobb2014bayesian, coull2015, bobb2018statistical}. Other examples of Bayesian methods in environmental mixtures exist, as well. Bayesian hierarchical methods \citep{maclehose2007bayesian, maclehose2014applications, furlong2017prenatal}, Bayesian model averaging \citep{fragoso2018bayesian, wilson2018model, berger2018associations, berger2018prenatal, berger2018associations2}, Bayesian additive regression trees \citep{park2014environmental, chipman2010bart, ko2016classification}, Bayesian profile regression \citep{coker2018multi, coker2017association, molitor2010bayesian}, and semi-Bayesian methods (which provide faster results) \citep{mak13_org,kalkbrenner2010perinatal,momoli2010analysis} have been implemented in environmental mixtures research, but they are not yet widely used. Computational advances in processing speed coupled with developments in machine learning and biostatistical modeling can make these methods accessible to environmental epidemiologists. There is space and need for more methods development, in collaboration with data and computer scientists and biostatisticians, in our field.

\section{Discussion}\label{sec:Discuss}

Although multiple methods currently exist for environmental mixtures research, no method can answer all mixtures questions, highlighting the importance of a well-defined research question to guide method selection. The interpretability of results (over predictive accuracy) is critical in determining the usefulness of novel statistical, data science, or machine learning methods in environmental epidemiology.

Despite statistical advances, all methods share certain limitations. Given high correlations across chemicals and varying measurement error in species-specific concentrations, any statistical method will pick the chemical with the least amount of measurement error that either is the toxic agent or is correlated with the toxic agent (but measured with less error) \citep{carroll2006measurement, pollack2012correlated}. Furthermore, if exposure biomarkers are used (e.g., chemicals or metabolites measured in biosamples), their half-lives and the timing of sample collection with respect to exposure matter. Depending on the chemicalâ€™s half-life and the critical window of exposure, all approaches are susceptible to selecting a chemical whose concentration was high during the critical exposure window and remained high during sampling; exposure to this selected chemical likely co-occurred with exposure to the actual toxic agent that---if it has a short half-life---might be undetected at sampling or measured with excess noise depending on the varying time between the critical exposure window and sampling across subjects. It is also conceivable that the actual toxic agent is not included in the mixture to be analyzed. Focusing, therefore, on identifying the toxic agent(s) might lead to the wrong conclusion under such scenarios, regardless of the choice (and performance) of method.

These issues may be amplified when the examined mixture is small, due to residual confounding from unmeasured chemicals or shared sources. Caution should also be applied when using the terms ``overall'' or ``cumulative'' for small mixtures, as these are usually only be a subset of the actual mixture of interest. The complexity of environmental mixtures---chemical and non-chemical---and analytical limitations for measurement of chemicals add to the difficulty of arriving at a perfectly-specified model. Including correlated exposure variables in any model may amplify rather than reduce confounding bias \citep{weisskopf2018bias}. Finally, uncertainty propagation is an often overlooked concern, mostly of unsupervised methods. Many researchers simply include PCA scores or cluster membership in health models ignoring the uncertainty inherent in the solution selection, often based on implicit assumptions. Propagation of uncertainty will lead to more valid inferences and can result in fewer spurious results and more consistent findings across methods and studies \citep{mak14_unc}.

In future mixtures analyses and methods development, researchers should focus on robustness of findings. Different populations experience different exposure mixtures and different distributions of potential modifiers, so we should not expect to replicate results (patterns or effect estimates) across populations. Rather, unstable methods should be avoided, and multiple methods should be used, whenever possible, to address a research question. When investigating an overall effect using WQS, for example, BKMR may be used as sensitivity analysis. Care should be taken, however, when employing different methods---if a specific research question is not stated, different methods may provide results that appear conflicting. For methods that employ simulations or rely on user-specified prior information (i.e., Bayesian methods), internal assessment of reproducibility is also warranted. 

These limitations and model-specific assumptions should be carefully considered when interpreting results of mixtures analyses. Additionally, groups developing mixtures methods should consider extensions that take this information into account when estimating health effects. Furthermore, combining methods may be of interest, for example coupling factor analysis with BKMR if one is interested in assessing the exposure-response of exposure patterns and their potentially non-linear interactions. 

Bayesian approaches, furthermore, inherently accommodate supervised pattern recognition, fully propagating uncertainty in the health model, thus identifying patterns specific to each outcome and better characterizing biological pathways. New Bayesian (and semi-Bayesian) methods could further allow more flexible modeling, explicit incorporation of uncertainty, inclusion of prior knowledge, and the ability to answer multiple questions simultaneously. Methods development should involve direct collaboration with computer scientists, data scientists, and biostatisticians to take advantage of computationally efficient machine learning algorithms and to obtain interpretable results from sophisticated models. Complex machine learning prediction methods generate enthusiasm across disciplines, but if their results are not directly interpretable in health effects analyses, they are unlikely to benefit the ultimate research goals of understanding biological pathways and informing regulatory action.

With careful incorporation of machine learning and data science methods, environmental epidemiologists are better able to explore complex relationships between environmental mixtures and adverse health. While each new prediction method appears to improve upon previous methods, effect estimation rather than outcome prediction should be the desired result. To this end, environmental epidemiologists must work with experts outside of our field to better adapt machine learning methods to our goals, instead of simply employing methods as they come. As methods development for environmental mixtures continues, we recommend Bayesian methods for their flexibility and interpretability of their results. Although no single model to date can answer all mixtures questions, a well-defined research question will point toward the correct approach---whether identification of patterns or independent, synergistic, or overall effect(s). Results are only useful, no matter how sophisticated the method, if they are robust, reproducible, interpretable and, finally, actionable.\footnotemark

\footnotetext{End of published work.}

\section{Dissertation overview}\label{sec:diss_over}

Environmental mixtures are an emerging topic in environmental health, and researchers agree that no single statistical or machine learning method can answer all potential questions in this area. The work presented in this dissertation focuses on a single mixture-related question: Are there specific patterns of chemical exposure in the study population? Because most dimensionality reduction methods employed in environmental epidemiology were not designed for this field, pattern recognition in  multi-pollutant exposure analysis contains several knowledge gaps. This dissertation details work to design, adapt, and apply statistical and machine learning methods to address environmental research questions.

In \textbf{Chapter~\ref{sec:ch2}} we introduce principal component pursuit (PCP), a robust dimensionality reduction method used in computer vision. PCP decomposes a chemical exposure matrix into a low rank matrix that captures consistent patterns in the data and a sparse matrix that isolates unique events \citep{candes2011robust,zhou2010stable}. In this work, we adapted PCP to better accommodate environmental data. We included a non-negativity constraint on the low rank matrix and a novel penalty for values below the analytic limit of detection. We altered the algorithm to allow for missing values, and we specified a cross-validation procedure to choose the optimal rank of the low rank matrix. We compared PCP and PCA performance on simulated data, and we used PCP to separate consistent patterns from unique events in a mixture of persistent organic pollutants (POP) measured in the 2001--2002 National Health and Nutrition Examination Surveys (NHANES).

In \textbf{Chapter~\ref{sec:ch3}} we introduce Bayesian non-parametric non-negative matrix factorization (\bnmfc), adapted from work in the geological sciences built on previous NMF modeling techniques in machine learning \citep{holtzman2018machine, lee1999learning, cemgil2008bayesian, paisley2014bayesian}. \bnmf includes several features well-suited to environmental data. \bnmf generates all estimated values from Gamma distributions, which contain only non-negative real numbers ($\mathbb R_{\geq 0}^+$), and a non-parametric prior on the number of patterns chooses the optimal number from the data. We extended \bnmf by explicitly modeling the uncertainty in pattern identification. In \textbf{Chapter~\ref{sec:ch4}} we incorporated this uncertainty quantification into a novel hierarchical health model. 

The work presented in this dissertation aims to contribute to the development of robust methods to identify patterns of exposure in chemical mixtures. We addressed several key gaps in environmental mixture research. (1) The true number of patterns in a mixture is never known, and common methods employed by environmental health researchers require specification or selection of the correct number by the researcher. (2) Chemical concentrations are non-negative, and methods that return solutions containing negative values discard this information, making the solution less interpretable. (3) To our knowledge, no methods used for pattern identification in environmental mixtures quantify the uncertainty in this step; nevertheless, it is  often the first step of a two-stage process to estimate associations between identified patterns and health outcomes. With this work, we present two novel methods for pattern identification and aim to contribute to a better understanding of environmental mixture research and the type of questions that these tools can answer.

\clearpage
